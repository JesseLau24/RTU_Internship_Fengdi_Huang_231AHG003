{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93baf16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Resuming from page 0 with after = None\n",
      "✅ Saved page_0.json with 25 new posts.\n",
      "✅ Saved page_1.json with 25 new posts.\n",
      "✅ Saved page_2.json with 25 new posts.\n",
      "✅ Saved page_3.json with 25 new posts.\n",
      "✅ Saved page_4.json with 25 new posts.\n",
      "✅ Saved page_5.json with 25 new posts.\n",
      "✅ Saved page_6.json with 25 new posts.\n",
      "✅ Saved page_7.json with 25 new posts.\n",
      "✅ Saved page_8.json with 25 new posts.\n",
      "✅ Saved page_9.json with 25 new posts.\n",
      "✅ Saved page_10.json with 25 new posts.\n",
      "✅ Saved page_11.json with 25 new posts.\n",
      "✅ Saved page_12.json with 25 new posts.\n",
      "✅ Saved page_13.json with 25 new posts.\n",
      "✅ Saved page_14.json with 25 new posts.\n",
      "✅ Saved page_15.json with 25 new posts.\n",
      "✅ Saved page_16.json with 25 new posts.\n",
      "✅ Saved page_17.json with 25 new posts.\n",
      "✅ Saved page_18.json with 25 new posts.\n",
      "✅ Saved page_19.json with 25 new posts.\n",
      "✅ Saved page_20.json with 25 new posts.\n",
      "✅ Saved page_21.json with 25 new posts.\n",
      "✅ Saved page_22.json with 25 new posts.\n",
      "✅ Saved page_23.json with 25 new posts.\n",
      "✅ Saved page_24.json with 25 new posts.\n",
      "✅ Saved page_25.json with 25 new posts.\n",
      "✅ Saved page_26.json with 25 new posts.\n",
      "✅ Saved page_27.json with 25 new posts.\n",
      "✅ Saved page_28.json with 25 new posts.\n",
      "✅ Saved page_29.json with 25 new posts.\n",
      "✅ Saved page_30.json with 25 new posts.\n",
      "✅ Saved page_31.json with 25 new posts.\n",
      "✅ Saved page_32.json with 25 new posts.\n",
      "✅ Saved page_33.json with 25 new posts.\n",
      "✅ Saved page_34.json with 25 new posts.\n",
      "✅ Saved page_35.json with 25 new posts.\n",
      "✅ Saved page_36.json with 25 new posts.\n",
      "✅ Saved page_37.json with 25 new posts.\n",
      "✅ Saved page_38.json with 25 new posts.\n",
      "✅ Saved page_39.json with 19 new posts.\n",
      "✅ No more pages. Stopping.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Referer\": \"https://www.reddit.com/r/Riga/\",\n",
    "}\n",
    "\n",
    "# === Config Path ===\n",
    "json_dir = \"Riga_JSON\"\n",
    "post_log_file = os.path.join(json_dir, \"saved_post_ids.txt\")\n",
    "after_checkpoint_file = os.path.join(json_dir, \"after_checkpoint.txt\")\n",
    "page_index_file = os.path.join(json_dir, \"page_index.txt\")\n",
    "\n",
    "TOTAL_PAGES = 200000\n",
    "os.makedirs(json_dir, exist_ok=True)\n",
    "\n",
    "# === Load saved post_ids ===\n",
    "saved_post_ids = set()\n",
    "if os.path.exists(post_log_file):\n",
    "    with open(post_log_file, \"r\") as f:\n",
    "        saved_post_ids = set(line.strip() for line in f)\n",
    "\n",
    "# === Load pagination parameters ===\n",
    "after = None\n",
    "if os.path.exists(after_checkpoint_file):\n",
    "    with open(after_checkpoint_file, \"r\") as f:\n",
    "        after = f.read().strip()\n",
    "\n",
    "page = 0\n",
    "if os.path.exists(page_index_file):\n",
    "    with open(page_index_file, \"r\") as f:\n",
    "        page = int(f.read().strip())\n",
    "\n",
    "print(f\"📌 Resuming from page {page} with after = {after}\")\n",
    "\n",
    "# === Start scraping pages ===\n",
    "for _ in range(page, TOTAL_PAGES):\n",
    "    url = \"https://www.reddit.com/r/Riga/.json\"\n",
    "    if after:\n",
    "        url += f\"?after={after}\"\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, headers=headers)\n",
    "        data = res.json()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching page {page}: {e}\")\n",
    "        time.sleep(5)\n",
    "        continue\n",
    "\n",
    "    children = data.get(\"data\", {}).get(\"children\", [])\n",
    "    after = data.get(\"data\", {}).get(\"after\")\n",
    "\n",
    "    new_posts = []\n",
    "    for child in children:\n",
    "        post_id = child[\"data\"].get(\"id\")\n",
    "        if post_id and post_id not in saved_post_ids:\n",
    "            new_posts.append(child)\n",
    "            saved_post_ids.add(post_id)\n",
    "\n",
    "    if not new_posts:\n",
    "        print(f\"⚠️ Page {page} contains only duplicate posts. Skipping.\")\n",
    "    else:\n",
    "        json_filename = f\"page_{page}.json\"\n",
    "        save_data = {\"data\": {\"children\": new_posts}}\n",
    "        with open(os.path.join(json_dir, json_filename), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(save_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Updae log and checkpoint\n",
    "        with open(post_log_file, \"a\") as f:\n",
    "            for post in new_posts:\n",
    "                f.write(post[\"data\"][\"id\"] + \"\\n\")\n",
    "\n",
    "        print(f\"✅ Saved {json_filename} with {len(new_posts)} new posts.\")\n",
    "\n",
    "    with open(after_checkpoint_file, \"w\") as f:\n",
    "        f.write(after if after else \"\")\n",
    "\n",
    "    page += 1\n",
    "    with open(page_index_file, \"w\") as f:\n",
    "        f.write(str(page))\n",
    "\n",
    "    if not after:\n",
    "        print(\"✅ No more pages. Stopping.\")\n",
    "        break\n",
    "\n",
    "    time.sleep(random.uniform(10, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a45b238f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ page_7.json - Post 1im7a5f -> 0 comments collected so far\n",
      "✅ page_7.json - Post 1ilb83v -> 11 comments collected so far\n",
      "✅ page_7.json - Post 1ikr7zl -> 13 comments collected so far\n",
      "✅ page_7.json - Post 1ikk5xq -> 37 comments collected so far\n",
      "✅ page_7.json - Post 1ik4b08 -> 39 comments collected so far\n",
      "✅ page_7.json - Post 1ijxhi5 -> 43 comments collected so far\n",
      "✅ page_7.json - Post 1ijt1ep -> 47 comments collected so far\n",
      "✅ page_7.json - Post 1ijr8dw -> 47 comments collected so far\n",
      "✅ page_7.json - Post 1ii5x64 -> 53 comments collected so far\n",
      "✅ page_7.json - Post 1ii2ni1 -> 84 comments collected so far\n",
      "✅ page_7.json - Post 1ii2u1x -> 117 comments collected so far\n",
      "✅ page_7.json - Post 1ii6hbk -> 119 comments collected so far\n",
      "✅ page_7.json - Post 1ihs730 -> 123 comments collected so far\n",
      "✅ page_7.json - Post 1ihkp4o -> 127 comments collected so far\n",
      "✅ page_7.json - Post 1ihebro -> 150 comments collected so far\n",
      "✅ page_7.json - Post 1ihewqh -> 158 comments collected so far\n",
      "✅ page_7.json - Post 1ihcs7y -> 167 comments collected so far\n",
      "✅ page_7.json - Post 1ihbnwu -> 168 comments collected so far\n",
      "✅ page_7.json - Post 1igoc78 -> 168 comments collected so far\n",
      "✅ page_7.json - Post 1igoz45 -> 170 comments collected so far\n",
      "✅ page_7.json - Post 1igbpg1 -> 172 comments collected so far\n",
      "✅ page_7.json - Post 1ifxmra -> 177 comments collected so far\n",
      "✅ page_7.json - Post 1ift29h -> 178 comments collected so far\n",
      "✅ page_7.json - Post 1if82au -> 198 comments collected so far\n",
      "✅ page_7.json - Post 1ienqqr -> 201 comments collected so far\n",
      "📁 Finished page_7.json -> Saved to page_7.csv\n",
      "✅ page_8.json - Post 1ieor74 -> 4 comments collected so far\n",
      "✅ page_8.json - Post 1ie8xj3 -> 6 comments collected so far\n",
      "✅ page_8.json - Post 1idwwmv -> 8 comments collected so far\n",
      "✅ page_8.json - Post 1ic5ogh -> 14 comments collected so far\n",
      "✅ page_8.json - Post 1ibvwxw -> 20 comments collected so far\n",
      "✅ page_8.json - Post 1ibhw9d -> 24 comments collected so far\n",
      "✅ page_8.json - Post 1iakdoo -> 34 comments collected so far\n",
      "✅ page_8.json - Post 1iakojs -> 34 comments collected so far\n",
      "✅ page_8.json - Post 1i9jxx1 -> 34 comments collected so far\n",
      "✅ page_8.json - Post 1i8sd2s -> 37 comments collected so far\n",
      "✅ page_8.json - Post 1i8rwmw -> 39 comments collected so far\n",
      "✅ page_8.json - Post 1i8ycvf -> 56 comments collected so far\n",
      "✅ page_8.json - Post 1i8d5u1 -> 59 comments collected so far\n",
      "✅ page_8.json - Post 1i8ix0t -> 62 comments collected so far\n",
      "✅ page_8.json - Post 1i86oki -> 67 comments collected so far\n",
      "✅ page_8.json - Post 1i7cawj -> 77 comments collected so far\n",
      "✅ page_8.json - Post 1i7cobc -> 80 comments collected so far\n",
      "✅ page_8.json - Post 1i6pbxd -> 86 comments collected so far\n",
      "✅ page_8.json - Post 1i6fy7o -> 96 comments collected so far\n",
      "✅ page_8.json - Post 1i6geo3 -> 100 comments collected so far\n",
      "✅ page_8.json - Post 1i6etnf -> 101 comments collected so far\n",
      "✅ page_8.json - Post 1i5vvud -> 103 comments collected so far\n",
      "✅ page_8.json - Post 1i5sww9 -> 106 comments collected so far\n",
      "✅ page_8.json - Post 1i5ot6g -> 106 comments collected so far\n",
      "✅ page_8.json - Post 1i5s13v -> 126 comments collected so far\n",
      "📁 Finished page_8.json -> Saved to page_8.csv\n",
      "✅ page_9.json - Post 1i46q1t -> 1 comments collected so far\n",
      "✅ page_9.json - Post 1i44yp3 -> 2 comments collected so far\n",
      "✅ page_9.json - Post 1i3rmas -> 3 comments collected so far\n",
      "✅ page_9.json - Post 1i3rh42 -> 15 comments collected so far\n",
      "✅ page_9.json - Post 1i30k20 -> 15 comments collected so far\n",
      "✅ page_9.json - Post 1i2vn9s -> 20 comments collected so far\n",
      "✅ page_9.json - Post 1i2hgfm -> 35 comments collected so far\n",
      "✅ page_9.json - Post 1i2efdf -> 37 comments collected so far\n",
      "✅ page_9.json - Post 1i2sj5r -> 40 comments collected so far\n",
      "✅ page_9.json - Post 1i23dqs -> 44 comments collected so far\n",
      "✅ page_9.json - Post 1i1r81w -> 55 comments collected so far\n",
      "✅ page_9.json - Post 1i1v6dt -> 64 comments collected so far\n",
      "✅ page_9.json - Post 1i161zo -> 69 comments collected so far\n",
      "✅ page_9.json - Post 1i0laq9 -> 82 comments collected so far\n",
      "✅ page_9.json - Post 1hzpi30 -> 85 comments collected so far\n",
      "✅ page_9.json - Post 1hzjkvk -> 86 comments collected so far\n",
      "✅ page_9.json - Post 1hyzfhw -> 91 comments collected so far\n",
      "✅ page_9.json - Post 1hy0we6 -> 95 comments collected so far\n",
      "✅ page_9.json - Post 1hxewwe -> 104 comments collected so far\n",
      "✅ page_9.json - Post 1hx9n4l -> 110 comments collected so far\n",
      "✅ page_9.json - Post 1hvdc86 -> 116 comments collected so far\n",
      "✅ page_9.json - Post 1hv223k -> 119 comments collected so far\n",
      "✅ page_9.json - Post 1hutsye -> 122 comments collected so far\n",
      "✅ page_9.json - Post 1hu6i80 -> 126 comments collected so far\n",
      "✅ page_9.json - Post 1htrbdp -> 126 comments collected so far\n",
      "📁 Finished page_9.json -> Saved to page_9.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "\n",
    "# === Config Path ===\n",
    "json_dir = \"Riga_JSON\"\n",
    "output_dir = \"Riga_CSV\"\n",
    "processed_file = os.path.join(output_dir, \"processed_pages.txt\")\n",
    "error_dir = os.path.join(output_dir, \"errors\")\n",
    "\n",
    "# === Create error_dir if doesn't exist ===\n",
    "os.makedirs(error_dir, exist_ok=True)\n",
    "\n",
    "# === Initialize processed pages set ===\n",
    "processed_pages = set()\n",
    "if os.path.exists(processed_file):\n",
    "    with open(processed_file, \"r\") as f:\n",
    "        processed_pages = set(line.strip() for line in f)\n",
    "\n",
    "# === Headers for HTTP requests ===\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Referer\": \"https://www.reddit.com/r/Riga/\",\n",
    "}\n",
    "\n",
    "# === Recursive function: extract all levels of comments ===\n",
    "def extract_comments(comments_list, post_id, all_comments):\n",
    "    for c in comments_list:\n",
    "        if c.get(\"kind\") != \"t1\":\n",
    "            continue\n",
    "        data = c[\"data\"]\n",
    "        comment = {\n",
    "            \"post_id\": post_id,\n",
    "            \"author\": data.get(\"author\"),\n",
    "            \"body\": data.get(\"body\", \"\").replace(\"\\n\", \" \"),\n",
    "            \"score\": data.get(\"score\"),\n",
    "            \"created_utc\": data.get(\"created_utc\"),\n",
    "            \"date\": datetime.utcfromtimestamp(data.get(\"created_utc\")).strftime('%Y-%m-%d %H:%M:%S') if data.get(\"created_utc\") else \"\"\n",
    "        }\n",
    "        all_comments.append(comment)\n",
    "        # Recursive function for child comments\n",
    "        if data.get(\"replies\") and isinstance(data[\"replies\"], dict):\n",
    "            replies = data[\"replies\"][\"data\"][\"children\"]\n",
    "            extract_comments(replies, post_id, all_comments)\n",
    "\n",
    "# === Iterate through all JSON files ===\n",
    "for filename in os.listdir(json_dir):\n",
    "    if not filename.endswith(\".json\") or filename in processed_pages:\n",
    "        continue\n",
    "\n",
    "    json_path = os.path.join(json_dir, filename)\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        try:\n",
    "            data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "    comments_data = []\n",
    "    for child in data.get(\"data\", {}).get(\"children\", []):\n",
    "        post_id = child[\"data\"].get(\"id\")\n",
    "        if not post_id:\n",
    "            continue\n",
    "\n",
    "        comment_url = f\"https://www.reddit.com/comments/{post_id}.json\"\n",
    "\n",
    "        try:\n",
    "            res = requests.get(comment_url, headers=headers)\n",
    "            if res.status_code == 429:\n",
    "                wait = int(res.headers.get(\"Retry-After\", 10))\n",
    "                print(f\"⏳ Rate limited on post {post_id}, waiting {wait} seconds...\")\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "\n",
    "            if res.status_code != 200:\n",
    "                print(f\"⚠️ Skipping post {post_id}, status code {res.status_code}\")\n",
    "                continue\n",
    "\n",
    "            if not res.text.strip():\n",
    "                print(f\"⚠️ Empty response for post {post_id}, possible rate limit or server error\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                thread_data = res.json()\n",
    "            except json.JSONDecodeError:\n",
    "                error_path = os.path.join(error_dir, f\"error_{post_id}.html\")\n",
    "                with open(error_path, \"w\", encoding=\"utf-8\") as ef:\n",
    "                    ef.write(res.text)\n",
    "                print(f\"❌ Error parsing JSON for post {post_id}, saved raw response to {error_path}\")\n",
    "                continue\n",
    "\n",
    "            if not isinstance(thread_data, list) or len(thread_data) < 2:\n",
    "                print(f\"⚠️ Invalid structure for post {post_id}, skipping\")\n",
    "                continue\n",
    "\n",
    "            comments_list = thread_data[1][\"data\"][\"children\"]\n",
    "            extract_comments(comments_list, post_id, comments_data)\n",
    "            print(f\"✅ {filename} - Post {post_id} -> {len(comments_data)} comments collected so far\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching comments for post {post_id}: {e}\")\n",
    "\n",
    "        time.sleep(random.uniform(1, 2))  # aviod rate limiting\n",
    "\n",
    "    # Save to CSV files correspondingly\n",
    "    csv_filename = filename.replace(\".json\", \".csv\")\n",
    "    csv_path = os.path.join(output_dir, csv_filename)\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        fieldnames = [\"post_id\", \"author\", \"body\", \"score\", \"created_utc\", \"date\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in comments_data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "    # Mark as processed\n",
    "    with open(processed_file, \"a\") as f:\n",
    "        f.write(f\"{filename}\\n\")\n",
    "\n",
    "    print(f\"📁 Finished {filename} -> Saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ec4a957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Total comment rows: 5638\n",
      "🔁 Duplicate bodies: 198\n",
      "✅ Unique bodies: 5440\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Config CSV file directory\n",
    "csv_dir = \"Riga_CSV\"\n",
    "\n",
    "# Collect all CSV file directories\n",
    "csv_files = [os.path.join(csv_dir, f) for f in os.listdir(csv_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "# Merge all CSV files\n",
    "all_data = pd.concat([pd.read_csv(f, usecols=[\"body\"]) for f in csv_files], ignore_index=True)\n",
    "\n",
    "# Summarize\n",
    "total_rows = len(all_data)\n",
    "unique_bodies = all_data[\"body\"].drop_duplicates()\n",
    "unique_rows = len(unique_bodies)\n",
    "duplicate_rows = total_rows - unique_rows\n",
    "\n",
    "# Print results\n",
    "print(f\"📄 Total comment rows: {total_rows}\")\n",
    "print(f\"🔁 Duplicate bodies: {duplicate_rows}\")\n",
    "print(f\"✅ Unique bodies: {unique_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "540b8164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded page_0.csv with 75 rows.\n",
      "✅ Loaded page_1.csv with 159 rows.\n",
      "✅ Loaded page_10.csv with 123 rows.\n",
      "✅ Loaded page_11.csv with 142 rows.\n",
      "✅ Loaded page_12.csv with 257 rows.\n",
      "✅ Loaded page_13.csv with 256 rows.\n",
      "✅ Loaded page_14.csv with 215 rows.\n",
      "✅ Loaded page_15.csv with 181 rows.\n",
      "✅ Loaded page_16.csv with 248 rows.\n",
      "✅ Loaded page_17.csv with 439 rows.\n",
      "✅ Loaded page_18.csv with 215 rows.\n",
      "✅ Loaded page_19.csv with 270 rows.\n",
      "✅ Loaded page_2.csv with 48 rows.\n",
      "✅ Loaded page_20.csv with 400 rows.\n",
      "✅ Loaded page_21.csv with 244 rows.\n",
      "✅ Loaded page_22.csv with 66 rows.\n",
      "✅ Loaded page_23.csv with 135 rows.\n",
      "✅ Loaded page_24.csv with 0 rows.\n",
      "✅ Loaded page_25.csv with 89 rows.\n",
      "✅ Loaded page_26.csv with 64 rows.\n",
      "✅ Loaded page_27.csv with 93 rows.\n",
      "✅ Loaded page_28.csv with 88 rows.\n",
      "✅ Loaded page_29.csv with 25 rows.\n",
      "✅ Loaded page_3.csv with 231 rows.\n",
      "✅ Loaded page_30.csv with 71 rows.\n",
      "✅ Loaded page_31.csv with 68 rows.\n",
      "✅ Loaded page_32.csv with 38 rows.\n",
      "✅ Loaded page_33.csv with 78 rows.\n",
      "✅ Loaded page_34.csv with 97 rows.\n",
      "✅ Loaded page_35.csv with 91 rows.\n",
      "✅ Loaded page_36.csv with 27 rows.\n",
      "✅ Loaded page_37.csv with 55 rows.\n",
      "✅ Loaded page_38.csv with 115 rows.\n",
      "✅ Loaded page_39.csv with 72 rows.\n",
      "✅ Loaded page_4.csv with 222 rows.\n",
      "✅ Loaded page_5.csv with 150 rows.\n",
      "✅ Loaded page_6.csv with 38 rows.\n",
      "✅ Loaded page_7.csv with 201 rows.\n",
      "✅ Loaded page_8.csv with 126 rows.\n",
      "✅ Loaded page_9.csv with 126 rows.\n",
      "📊 Total merged rows (before deduplication): 5638\n",
      "🧹 Rows after deduplication: 5440\n",
      "📁 Merged file saved to: Riga_CSV\\may_7_merged_comments.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\AppData\\Local\\Temp\\ipykernel_12132\\796844495.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(all_dfs, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Config input and output directories\n",
    "input_dir = \"Riga_CSV\"\n",
    "output_file = os.path.join(input_dir, \"may_7_merged_comments.csv\")\n",
    "\n",
    "# Merge all CSV files\n",
    "all_dfs = []\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".csv\") and filename != \"may_7_merged_comments.csv\":\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_dfs.append(df)\n",
    "            print(f\"✅ Loaded {filename} with {len(df)} rows.\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to load {filename}: {e}\")\n",
    "\n",
    "# Merge and drop duplicates\n",
    "if all_dfs:\n",
    "    merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"📊 Total merged rows (before deduplication): {len(merged_df)}\")\n",
    "\n",
    "    # Drop duplicates based on \"body\" column\n",
    "    merged_df.drop_duplicates(subset=\"body\", inplace=True)\n",
    "    print(f\"🧹 Rows after deduplication: {len(merged_df)}\")\n",
    "\n",
    "    # Merge all processed CSV files\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"📁 Merged file saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"⚠️ No CSV files found or all failed to load.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bd16bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Comment count distribution by year:\n",
      "year\n",
      "2022      76\n",
      "2023     655\n",
      "2024    3306\n",
      "2025    1403\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the merged CSV file\n",
    "df = pd.read_csv(\"Riga_CSV/may_7_merged_comments.csv\")\n",
    "\n",
    "# Ensure the 'date' column is of datetime type\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Drop rows with unparseable dates\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Extract the year\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# Count frequency by year\n",
    "year_counts = df['year'].value_counts().sort_index()\n",
    "\n",
    "print(\"📅 Comment count distribution by year:\")\n",
    "print(year_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39559b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Comment count distribution by year and month:\n",
      "month   1    2    3    4    5    6    7    8    9    10   11   12\n",
      "year                                                             \n",
      "2022     0    0    0    0    0    0    0    0    0    0   17   59\n",
      "2023    85   54   28   33   75   27  105   41   55   57   20   75\n",
      "2024    81   44   53   50   12  163  532  873  348  488  440  222\n",
      "2025   268  245  415  258  217    0    0    0    0    0    0    0\n"
     ]
    }
   ],
   "source": [
    "# Extract the month\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "# Count frequency by year and month\n",
    "year_month_counts = df.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "\n",
    "print(\"\\n📅 Comment count distribution by year and month:\")\n",
    "print(year_month_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8b28dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Comment count distribution for each day of every month in 2025:\n",
      "day           1     2     3     4     5     6     7     8     9     10    11    12    13    14    15    16    17    18    19    20    21    22    23    24    25    26   27   28   29    30    31\n",
      "year month                                                                                                                                                                                       \n",
      "2025 1       1.0   5.0   0.0  14.0   4.0   5.0   3.0   2.0  15.0   7.0   5.0   4.0   9.0   2.0  18.0  27.0  14.0   5.0   1.0  18.0  21.0  16.0   6.0  16.0   7.0   5.0  8.0  8.0  4.0   8.0  10.0\n",
      "     2       8.0  13.0  10.0  39.0  46.0   7.0  18.0  40.0  18.0   4.0   3.0   4.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   3.0  12.0   3.0   2.0  5.0  9.0  0.0   0.0   0.0\n",
      "     3       5.0   9.0  22.0   8.0  20.0   5.0   3.0   1.0  17.0  10.0   7.0  34.0   9.0  35.0  22.0  11.0  21.0   7.0   3.0  11.0  10.0   3.0  15.0   6.0  38.0  26.0  9.0  5.0  7.0  18.0  18.0\n",
      "     4      13.0   9.0   4.0   4.0  15.0   0.0   1.0   1.0   3.0  13.0  35.0  22.0  16.0  18.0  13.0   7.0  10.0  10.0  15.0  15.0   3.0   5.0   1.0   4.0   3.0   1.0  0.0  1.0  7.0   9.0   0.0\n",
      "     5       6.0   1.0   8.0  29.0  26.0  25.0   7.0   3.0  32.0   5.0   3.0   1.0   7.0  19.0   3.0   2.0   0.0   4.0   7.0   6.0  22.0   1.0   0.0   0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0\n"
     ]
    }
   ],
   "source": [
    "# Prevent truncation in display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Parse dates and drop unparseable entries\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df = df.dropna(subset=['date'])\n",
    "\n",
    "# Extract year, month, and day\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "\n",
    "# Filter data for the year 2022\n",
    "df_2025 = df[df['year'] == 2025]\n",
    "\n",
    "# Group and count comments by year, month, and day\n",
    "daily_counts = df_2025.groupby(['year', 'month', 'day']).size().reset_index(name='count')\n",
    "\n",
    "# Create pivot table: year + month as rows, day as columns\n",
    "pivot_table = daily_counts.pivot_table(\n",
    "    index=['year', 'month'], columns='day', values='count', fill_value=0\n",
    ")\n",
    "\n",
    "# Output the tidy table\n",
    "print(\"📅 Comment count distribution for each day of every month in 2025:\")\n",
    "print(pivot_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
